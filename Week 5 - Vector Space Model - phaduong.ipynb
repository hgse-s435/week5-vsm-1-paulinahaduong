{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Vector Space Model (VSM) and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the next weeks, we are going to re-implement Sherin's algorithm and apply it to the text data we've been working on last week! Here's our roadmap:\n",
    "\n",
    "**Week 5 - data cleaning**\n",
    "1. import the data\n",
    "2. clean the data (e.g., remopve stop words, punctuation, etc.)\n",
    "3. build a vocabulary for the dataset\n",
    "4. create chunks of 100 words, with a 25-words overlap\n",
    "5. create a word count matrix, where each chunk of a row and each column represents a word\n",
    "\n",
    "**Week 6 - vectorization and linear algebra**\n",
    "6. Dampen: weight the frequency of words (1 + log[count])\n",
    "7. Scale: Normalize weighted frequency of words\n",
    "8. Direction: compute deviation vectors\n",
    "\n",
    "**Week 7 - Clustering**\n",
    "9. apply different unsupervised machine learning algorithms\n",
    "    * figure out how many clusters we want to keep\n",
    "    * inspect the results of the clustering algorithm\n",
    "\n",
    "**Week 8 - Visualizing the results**\n",
    "10. create visualizations to compare documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) using glob, find all the text files in the \"Papers\" folder\n",
    "# Hint: refer to last week's notebook\n",
    "\n",
    "import glob \n",
    "txt_files = glob.glob('papers/*.txt')\n",
    "# print(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "\n",
      "\f",
      "towards closing the loop: bridging machine-induced\n",
      "pedagogical policies to learning theories\n",
      "guojing zhou, jianxun wang, collin f. lynch, min chi\n",
      "department of computer science\n",
      "north carolina state university\n",
      "raleigh, nc 27695\n",
      "\n",
      "{gzhou3,jwang75,cflynch,mchi}@ncsu.edu\n",
      "abstract\n",
      "in this study, we applied decision trees (dt) to extract\n",
      "a compact set of pedagogical decision-making rules from\n",
      "an original full set of 3,702 reinforcement learning (rl)induced rules, referred to as the dt-rl rules and full-rl\n",
      "rules respectively. we then evaluated the effectiveness of\n",
      "the two rule sets against a baseline random condition in\n",
      "which the tutor made random yet reasonable decisions. we\n",
      "explored two types of trees (weighted and unweighted) as\n",
      "well as two pruning strategies (pre- and post-pruning). we\n",
      "found that post-pruned weighted trees produced the best results with 529 dt-rl rules. the empirical evaluation was\n",
      "conducted in a classroom study using an existing intelligent\n",
      "tutoring system (its) named pyrenees. 153 students were\n",
      "randomly assigned to three conditions. the procedure was\n",
      "the same for all students with domain content and required\n",
      "steps strictly controlled. the only substantive differences\n",
      "between the three conditions were the policy: (full-rl vs.\n",
      "dt-rl vs. random). our result showed that as expected\n",
      "the machine induced policies (full-rl and dt-rl) are significantly more effective than the random policy; more importantly, no significant difference was found between the\n",
      "full-rl and dt-rl policies though the number of dt-rl\n",
      "rules is less than 15% of the number of the full-rl rules\n",
      "and the former group also took significantly less time than\n",
      "the latter.\n",
      "\n",
      "1.\n",
      "\n",
      "introduction\n",
      "\n",
      "intelligent tutoring systems (itss) are interactive e-learning\n",
      "environments that support students’ learning by providing\n",
      "instruction, scaffolded practice, and on-demand help. the\n",
      "system’s behaviors can be viewed as a sequential decisionmaking process where at each step the system chooses an\n",
      "appropriate action from a set of options. pedagogical strategies are the policies used to decide what action to take next\n",
      "in the face of alternatives. each system decision will affect\n",
      "the user’s subsequent actions and performance. its impact\n",
      "on outcomes cannot always be immediately observed and the\n",
      "effectiveness of each decision depends upon the effectiveness\n",
      "\n",
      "of subsequent actions. ideally, an effective learning environment will adapt its decisions to users’ specific needs [1, 11].\n",
      "however, there is no existing well-established theory on how\n",
      "to make these system decisions effectively. generally speaking, prior research on pedagogical policies can be divided\n",
      "into two general categories: top-down or theory-driven, and\n",
      "bottom-up or data-driven.\n",
      "in theory-driven approaches, itss employ hand-coded pedagogical rules that seek to implement existing cognitive or\n",
      "learning theories [1, 10, 17]. while existing learning literature gives helpful guidance on the design of pedagogical\n",
      "rules, such guidance is often too general to implement as\n",
      "effective immediate decisions. for example, the aptitudetreatment interaction (ati) theory states that instructors\n",
      "should match their interventions to the aptitude of the learner\n",
      "[5]. while the principle behind this theory is understandable, it is not clear how to implement that rule for each\n",
      "decision. how do we represent learner’s aptitude for each\n",
      "equation, how exact should be the system’s adaptation, and\n",
      "so on.\n",
      "data-driven approaches, on the other hand, derive pedagogical policies directly from prior data. here the policies\n",
      "specify the pedagogical decisions at a detailed level. reinforcement learning (rl), which we use here, is one popular\n",
      "approach that is able to derive pedagogical policies directly\n",
      "from student-system interaction logs. these policies are defined as a set of state-action mapping rules, which give the\n",
      "best decision to take in each state. the states are typically\n",
      "represented as sets of features and the actions are pedagogical actions such as presenting a worked example (we) or\n",
      "requiring the student to solve problems (ps). when the system presents a worked example, the students will be given a\n",
      "detailed example showing a complete expert solution for the\n",
      "problem or the best step to take given their current solution\n",
      "state. in problem solving, by contrast, students are tasked\n",
      "with solving a problem using the its or with completing an\n",
      "individual problem-solving step.\n",
      "for this project, our original complete rl-induced policy involves the following seven features representing the students’\n",
      "learning process from different perspectives1 .\n",
      "\n",
      "1\n",
      "in the format of: [feature-name] (discretization procedure): explanation of the feature.112\n",
      "\n",
      "\f",
      "1. [nwesinceps] (0 → 0; (0, 1] → 1; (1, +∞) → 2):\n",
      "the number of worked example (we) steps received\n",
      "since the last problem solving (ps) step.\n",
      "2. [timeinsession] ([0, 2290] → 0; (2290, 4775] → 1;\n",
      "(4775, 7939] → 2; (7939, +∞) → 3): the total time\n",
      "spent in the current session.\n",
      "3. [avgtimeonstepps] ([0, 29.01] → 0; (29.01,\n",
      "48.71] → 1; (48.71, +∞) → 2): the average amount of\n",
      "time spent on each ps step.\n",
      "4. [avgtimeonstepsessionps] ([0, 23.51] → 0;\n",
      "(23.51, 36.56] → 1; (36.56, 55] → 2; (55, +∞) → 3):\n",
      "the average amount of time spent on each ps step in\n",
      "the current session.\n",
      "5. [nstepsincelastwrongkc] ([0, 1] → 0; (1, 7]\n",
      "→ 1; (7, 25] → 2; (25, +∞) → 3): the number of steps\n",
      "received since the last wrong ps step on the current\n",
      "knowledge component (kc).\n",
      "6. [nwestepsincelastwrong] ([0, 1] → 0; (1, 4]\n",
      "→ 1; (4, 10] → 2; (10, +∞) → 3): the number of we\n",
      "steps since the last wrong ps step.\n",
      "7. [ncorrectpsstepsincelastwrongkcsession]\n",
      "(0 → 0; (0, 3] → 1; (3, 10] → 2; (10, +∞) → 3): the\n",
      "number of correct ps steps since the last wrong ps\n",
      "step on the current kc in the current session.\n",
      "with this feature set, a state can be represented as a 7dimensional vector where each element denotes a discretized\n",
      "feature value. then, the rules can then be represented as:\n",
      "(0:0:0:0:0:0:0) -> ps\n",
      "(0:0:0:0:0:0:1) -> ps\n",
      "(0:0:0:0:0:1:0) -> ps\n",
      "(0:0:0:0:0:1:1) -> we\n",
      "in this study we discretized the features into three-four values producing a seven-feature state. this results in a state\n",
      "space of 32 ∗ 45 = 9216, that is 9216 rules in one rl-induced\n",
      "policy. while these types of polices can specify the exact\n",
      "action to take in each case, they are usually too narrow to\n",
      "be aligned to existing learning theories. each of the rules\n",
      "covers only a very specific case and the relationship between\n",
      "rules is unknown. thus it is impossible to explain the power\n",
      "of those rules from the perspective of learning theory. the\n",
      "opacity of those induced rules not only hinders us in improving data-driven methodologies when they go wrong, it also\n",
      "prevents us from advancing learning science research more\n",
      "generally. moreover, it is possible that some of the decisions\n",
      "are environment-specific and may not generalize to other\n",
      "contexts. this in turn prevents translating these induced\n",
      "policies to environments other than the one from which they\n",
      "are induced. therefore, a general method is needed to shed\n",
      "some light on the extracted detailed data-driven policies.\n",
      "decision tree (dt) induction is a robust data mining approach which can be used to extract a compact set of rules\n",
      "from a set of specific examples. it builds a tree-like hierarchical decision-making pattern which represents the knowledge it learned. each path from root to leaf represents a\n",
      "single rule which may be dealt with separately. prior studies have shown that dts can match training examples in\n",
      "most cases, even with relatively small trees. davidson et\n",
      "\n",
      "al., for example, built a dt for predicting the extinction\n",
      "risk of mammals [6]. each of the species was described by\n",
      "11 ecological features (e.g body mass, geographic range and\n",
      "population density) and were labeled with their extinction\n",
      "risk (threatened vs. non-threatened). their tree contained\n",
      "20 general rules which covered 4500 training examples, with\n",
      "a decision accuracy over 80%. additionally, reinchard et al.\n",
      "built a dt for predicting the invasiveness of woody plants\n",
      "[13]. the resulting dt encoded 15 rules from 235 examples,\n",
      "with a decision accuracy over 76%. therefore, in our study,\n",
      "we will apply dt to extract general pedagogical decisionmaking rules from the detailed rl-induced policies.\n",
      "in short, our primary research question is: is dt an effective methodology for extracting more general pedagogical\n",
      "rules from the detailed rl-induced pedagogical rules? in order to investigate this question, we will build dts using the\n",
      "rules in a rl-induced policy as training examples and empirically evaluate the effectiveness of the extracted set of dt\n",
      "rules by comparing it to the full set of rl-induced rules in a\n",
      "classroom study. the state features in the rl-induced policies are the input features for the dt and the pedagogical\n",
      "actions are the output labels. in our empirical evaluation,\n",
      "we separate the pedagogical decisions from the instructional\n",
      "content, strictly controlling the content so that it is equivalent for all participants by 1) using an its which provides\n",
      "equal support for all learners; and 2) focusing on tutorial\n",
      "decisions that cover the same domain content, in this case\n",
      "we versus ps.\n",
      "\n",
      "2. background\n",
      "2.1 applying rl to itss\n",
      "beck et al. applied rl to induce pedagogical policies that\n",
      "would minimize the time students take to complete problems on animalwatch, an its for grade school arithmetic\n",
      "[2]. they trained the model with simulated students. the\n",
      "low cost of generated data allowed them to apply a modelfree rl method, temporal difference learning. during the\n",
      "test phase, the induced policies were added to animalwatch\n",
      "and the new system was empirically compared with the original system. their results showed that the policy group\n",
      "spent significantly less time per problem than their no-policy\n",
      "peers. note that their primary goal was to reduce the amount\n",
      "of time per problem, however faster problem-solving does\n",
      "not always result in better learning performance. nonetheless, their results showed that rl can be successfully applied\n",
      "to induce pedagogical policies for itss.\n",
      "iglesias et al., on the other hand, focused on applying rl to\n",
      "improve the effectiveness of an intelligent educational system that teaches students database design [8, 9]. they\n",
      "applied another model-free rl algorithm, q-learning to induce policies that provide students with direct navigation\n",
      "support through the system’s content. they used simulated\n",
      "students to induce the policy and empirically evaluated its\n",
      "effectiveness on real students. their results showed that\n",
      "while the policy led to more effective system usage behaviors from students, the policy students did not outperform\n",
      "the no-policy peers in terms of learning outcomes.\n",
      "shen investigated the impact of both immediate and delayed reward functions on rl-induced policies and empirically evaluated the effectiveness of the induced policies within113\n",
      "\n",
      "\f",
      "an intelligent tutoring system called deep thought [15].\n",
      "the induced pedagogical policies are used to decide whether\n",
      "the next task should be we or ps. they found that some\n",
      "learners benefited significantly more from effective pedagogical policies than others.\n",
      "finally, chi et al. applied model-based rl to induce pedagogical policies to improve the effectiveness of an intelligent\n",
      "natural language tutoring system for college-level physics\n",
      "called cordillera [4]. the authors collected an exploratory\n",
      "corpus by training human students on an its that makes\n",
      "random decisions and then applied rl to induce pedagogical policies from the corpus. they showed that the induced\n",
      "policies were significantly more effective than the prior ones.\n",
      "in short, prior studies have shown that rl-induced pedagogical policies can improve students’ learning or reduce\n",
      "training time. however, all of these studies focused on the\n",
      "effectiveness of the rl-induced policies. none of them considered extracting more general rules from the induced policies.\n",
      "\n",
      "2.2\n",
      "\n",
      "extracting general rules\n",
      "\n",
      "in addition to the work of davidson et al. [6] and reinchard\n",
      "et al. [13], dts have been used for other tasks. vayssiers\n",
      "et al., for example, applied classification and regression\n",
      "trees to predict the presence of 3 species of oak in california [18]. their training examples were vegetation type map\n",
      "records for 2085 unique locations. each record consisted of\n",
      "25 climatic and geographic features as well as 3 labels showing the presence of the species (quercus agrifolia, quercus\n",
      "douglasii and quercus lobata). one dt was induced for\n",
      "each type. the dts were tested on another dataset which\n",
      "contains the same type of records for 2016 locations. for\n",
      "quercus agrifolia, the induced tree had 10 leaf nodes and\n",
      "94.9% of its predictions are correct for the locations that\n",
      "have the presence of this oak (sensitivity) while 86.7% of\n",
      "its predictions are correct for cases without the oak (specificity). for quercus douglasii, the induced tree had 22 leaf\n",
      "nodes and a sensitivity and specificity of 87% and 79.9%\n",
      "respectively. for quercus lobata, the tree had 6 leaves but\n",
      "reached a sensitivity of 77% and a specificity of 73.3%.\n",
      "thus, prior studies have shown that dt can effectively extract a small set of general decision-making rules from a\n",
      "large set of specific examples. however, all the examples\n",
      "used by these studies were observations of existing phenomena. so far as we know, this work is the only relevant research on the application of dt to extract a compact set\n",
      "of decision-making rules directly from full rl-induced rules\n",
      "and empirically evaluated the two sets of the rules.\n",
      "\n",
      "2.3\n",
      "\n",
      "applying dt to rl\n",
      "\n",
      "prior research on incorporating dt with rl has largely\n",
      "focused on seeking a better representation of state space\n",
      "or policy for rl. boutilier et al [3]. proposed representational and computational techniques for markov decision\n",
      "processes (mdps) to reduce the size of the state space.\n",
      "they used dynamic bayesian networks and dts to represent stochastic actions as well as dts to represent rewards.\n",
      "based upon this representation, they then developed algorithms to find conditional optimal policies. their method\n",
      "was empirically evaluated on several planning problems and\n",
      "\n",
      "they showed significant savings in both time and space for\n",
      "some types of problems. gupta et al. proposed the policy\n",
      "tree algorithm for rl. this algorithm is designed to directly\n",
      "induce a functional representation of the conditional optimal\n",
      "policies as a dt. they evaluated it on a variety of domains\n",
      "and showed that it was able to make splits properly [7].\n",
      "in short, prior researchers have shown that properly combining dt with rl can result in a large amount of savings\n",
      "in time and space for finding good policies. however, none\n",
      "of these studies directly applied dt on rl-induced policies.\n",
      "\n",
      "3.\n",
      "\n",
      "induce full set of rl-policy\n",
      "\n",
      "previously, researchers have typically used the markov decision process (mdp) [16] framework to model user-system\n",
      "interactions. the central idea behind this approach is to\n",
      "transform the problem of inducing effective pedagogical policies on what action the agent should take to the problem of\n",
      "computing an optimal policy for an mdp.\n",
      "\n",
      "3.1 markov decision process\n",
      "an mdp is a mathematical framework for representing an\n",
      "rl task. it is defined by: a tuple hs , a, t , ri. where s =\n",
      "{s1 , s2 , ..., sn } denotes the state space; a = {a1 , a2 , ..., am }\n",
      "represents a set of agent’s possible actions; and t : s × a ×\n",
      "s → [0, 1] is a transition probability table, where each element is tsai sj = p(sj |si , a). this in turn indicates the\n",
      "probability of transiting from state si to state sj by taking an action a while r : s × a × s → r assigns rewards\n",
      "to state transitions given actions. the policy is defined as\n",
      "π : s → a, mapping state s into action a with the goal of\n",
      "maximizing the expected reward.\n",
      "after defining an mdp, we can transfer the student-system\n",
      "interaction dialog into the trajectory which can then be represented as follows:\n",
      "a ,r\n",
      "\n",
      "a ,r\n",
      "\n",
      "a ,r\n",
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "s1 −−1−−→\n",
      "s2 −−2−−→\n",
      "s3 −−3−−→\n",
      "... → sn\n",
      "\n",
      "a ,r\n",
      "\n",
      "i\n",
      "where si −−i−−→\n",
      "si+1 means that the tutor executed action\n",
      "ai and received reward ri in state si , and then transferred\n",
      "to the next state si+1 . in general, the reward can be divided\n",
      "into two categories, immediate and delayed, where immediate rewards are received during the state transition, and\n",
      "delayed are available after reaching to goal state.\n",
      "\n",
      "3.2 training datasets\n",
      "our training dataset was collected from three exploratory\n",
      "studies in which students were trained on an its which made\n",
      "random yet reasonable pedagogical decisions. the studies\n",
      "were given as homework assignments during csc226: discrete mathematics, a core cs course offered at ncsu during the fall 2014, spring 2015 and fall 2015 semesters. the\n",
      "dataset contains a total of 149 students’ interaction logs.\n",
      "all students used the same its, followed the same general\n",
      "procedure, studied the same training materials, and worked\n",
      "through the same training problems. in order to model the\n",
      "students’ learning process, we extracted a total of 142 state\n",
      "feature variables, which can be grouped into five categories:\n",
      "1. autonomy (am): the amount of work done by the student: such as the number of problems solved so far pscount\n",
      "or the number of hints requested hintcount.114\n",
      "\n",
      "\f",
      "2. temporal situation (ts): the time related information about the work process: such as the average time taken\n",
      "per problem avgtime, or the total time spent solving a problem totalpstime.\n",
      "3. problem solving (ps): information about the current\n",
      "problem solving context, such as the difficulty of the current\n",
      "problem probdiff, or whether the student changes the difficulty level newlevel.\n",
      "4. performance (pm): information about the student’s\n",
      "performance during problem solving: such as the number of\n",
      "right application of rules rightapp.\n",
      "5. student action (sa): the statistical measurement of\n",
      "student’s behavior: such as the number of non-empty-click\n",
      "actions that students take actioncount, or the number of\n",
      "clicks for derivation appcount.\n",
      "\n",
      "based methods and an ensemble method and capped the\n",
      "maximum number of state feature size to be eight. more\n",
      "details of our feature selection methods are described in [14].\n",
      "the final resulting rl policy involves seven state features\n",
      "and 3706 rules.\n",
      "\n",
      "3.3\n",
      "\n",
      "4.1 unweighted vs. weighted tree\n",
      "\n",
      "inducing rl policies\n",
      "\n",
      "in order to apply rl to induce pedagogical policies, we\n",
      "first defined the pedagogical decision-making problem as an\n",
      "mdp. the state representation includes all of the relevant\n",
      "features available at the beginning of each step. the actions are we and ps at the step level. the transition tables were calculated on our training dataset, and our reward\n",
      "function includes two types of reward: delayed and immediate. our most important reward is based on normalized\n",
      "), which measures the\n",
      "learning gain (nlg) ( posttest−pretest\n",
      "1−pretest\n",
      "students’ learning gains irrespective of their incoming competence. this reward was given as a delayed reward as nlg\n",
      "scores can only be calculated after students finish the entire\n",
      "training process. however, shen et al. [15] showed that giving immediate rewards can lead to the production of more\n",
      "effective policies when compared to delayed rewards. this\n",
      "is known as the credit-assignment problem. the more that\n",
      "we delay success measures from a series of sequential decisions, the more difficult it becomes to identify which of the\n",
      "decision(s) in the sequence are responsible for our final success or failure. therefore, for the purposes of this study we\n",
      "also assigned immediate rewards based upon the students’\n",
      "performance during training on the system.\n",
      "the value iteration algorithm was applied to find the optimal\n",
      "policy. this algorithm operates by finding the optimal value\n",
      "for each state v ∗ (s). the optimal value for a given state is\n",
      "the expected discounted reward that the agent will gain if\n",
      "it starts in s and follows the optimal policy to the goal.\n",
      "generally speaking, v ∗ (s) can be obtained by the optimal\n",
      "value function for each state-action pair q∗ (s, a) which is\n",
      "defined as the expected discounted reward the agent will\n",
      "gain if it takes an action a in a state s and follows the optimal\n",
      "policy to the end. the optimal state value v ∗ (s) and value\n",
      "function q∗ (s, a) can be obtained by iteratively updating\n",
      "v (s) and q(s, a) via equations 1 and 2 until they converge:\n",
      "x\n",
      "q(s, a) := r(s, a) + γ\n",
      "p(sj |si , a)v (s0 )\n",
      "(1)\n",
      "v (s)\n",
      "\n",
      ":=\n",
      "\n",
      "s0 ∈s\n",
      "\n",
      "max q(s, a)\n",
      "a\n",
      "\n",
      "(2)\n",
      "\n",
      "here, p(sj |si , a) is the estimated transition model t , r(s, a)\n",
      "is the estimated reward model and 0 ≤ γ ≤ 1 is a discount\n",
      "factor.\n",
      "to induce effective pedagogical policies, we combined rl\n",
      "with various feature selections including 10 types of correlation-\n",
      "\n",
      "4.\n",
      "\n",
      "extracting compact dt-rl sets\n",
      "\n",
      "in order to extract a more compact set of decision-making\n",
      "rules from the full set of rl-induced rules, we implemented\n",
      "the id3 algorithm to build dts [12]. each rule in the final\n",
      "rl-induced policy was used as a training example. two\n",
      "types of decision trees were built: unweighted and weighted,\n",
      "as well as two types of pruning strategies were implemented:\n",
      "pre- and post-pruning. next, we will discuss each of them\n",
      "in turn.\n",
      "\n",
      "the decision to give a we vs. ps may impact students’\n",
      "learning differently in different situations. we therefore built\n",
      "two types of decision trees: unweighted and weighted. unweighted trees treated each decision equally while weighted\n",
      "trees take account of the relative importance of each pedagogical rule. when applying the value iteration algorithm\n",
      "to induce the optimal policy, we generate the optimal value\n",
      "function q∗ (s, a), which gives the expected discounted reward each agent will gain if it takes an action a in a state s\n",
      "and follows the optimal policy to the end. for a given state\n",
      "s, a large difference between the values of q(s, “p s”) and\n",
      "q(s, “w e”) indicates that it is more important for the its\n",
      "to follow the optimal decision in the state s. we therefore\n",
      "used the absolute difference between the q values for each\n",
      "state s to weight each rl pedagogical rule.\n",
      "the id3 algorithm builds a tree recursively from root to\n",
      "leaves. on each iteration of the construction process the\n",
      "algorithm will check the state of the dataset for the current\n",
      "branch. it will then select a test feature for the current\n",
      "node based upon the weighted information gain. the current\n",
      "node will then be expanded by adding branches to it, each\n",
      "of which represents a possible value for the selected feature.\n",
      "the data will be partitioned over the branches according to\n",
      "the value of the test feature. the selected feature cannot\n",
      "be used again by its children. weighted information gain is\n",
      "defined by the difference between the weighted entropy of the\n",
      "examples before it is selected and after they are separated\n",
      "by feature value. the weighted entropy of a node can be\n",
      "calculated by equation 3\n",
      "h(g) = −\n",
      "\n",
      "j\n",
      "x\n",
      "\n",
      "p(i|g)log2 p(i|g)\n",
      "\n",
      "(3)\n",
      "\n",
      "i=1\n",
      "\n",
      "j is the total number of output label classes. in our case,\n",
      "it is the number of pedagogical actions (we or ps) which\n",
      "is 2 . p(i|g) is the\n",
      "weighted frequency defined by the equap\n",
      "p\n",
      "w\n",
      "tion: p(i|g) = p x∈i wxy .\n",
      "x∈i wx is the total weight of the\n",
      "y∈g\n",
      "examples\n",
      "which\n",
      "are\n",
      "in\n",
      "node\n",
      "g and which belong to class i.\n",
      "p\n",
      "and y∈g wy is the total weights of examples in node g.\n",
      "the information gain of spliting the current set of training\n",
      "examples using feature f can be calculated by equation 4:\n",
      "ig(f, g) = h(g) −k\n",
      "x\n",
      "j=1\n",
      "\n",
      "p(tj |g)h(tj )\n",
      "\n",
      "(4)\n",
      "\n",
      "115\n",
      "\n",
      "\f",
      "p(tj |g) is the\n",
      "weighted frequency of the examples in node g:\n",
      "p\n",
      "p\n",
      "xf =t,x∈g wx\n",
      "p\n",
      "p(tj |g) =\n",
      ".\n",
      "xf =t,x∈g wx is the total weights\n",
      "y∈g wy\n",
      "of\n",
      "examples\n",
      "in\n",
      "nodes\n",
      "g\n",
      "whose\n",
      "value of feature f is j and\n",
      "p\n",
      "y∈g wy is the total weight of examples in nodes g.\n",
      "\n",
      "4.2\n",
      "\n",
      "pre-pruning and post-pruning\n",
      "\n",
      "to control the size of rules induced by dt, we examined\n",
      "two types of pruning strategy: pre- and post-pruning. the\n",
      "pre-pruning is conducted during the process of building the\n",
      "tree and it used the information gain to determine whether\n",
      "to expand or to terminate. only nodes with an information\n",
      "gain greater than a threshold times its depth: ig(f, g) ≥\n",
      "θ × dg will be expanded and others will be made as a leaf.\n",
      "θ is a fixed threshold and dg is the depth of node g.\n",
      "post-pruning is conducted after the whole decision tree is\n",
      "built and it used the error rate as the pruning measure. the\n",
      "error\n",
      "rate before a node is expanded is defined as: eg =\n",
      "p\n",
      "i∈i wi\n",
      ". i is the set of the decisions incorrectly classified\n",
      "|g|\n",
      "by node g and |g| is the total number of examples in the\n",
      "node g. the\n",
      "p error\n",
      "p rate after a node is expanded is defined\n",
      "wi\n",
      "as: ec = c∈c |g|j∈ic . c is the set of children nodes\n",
      "of g after it is expanded and ic is the set of the decisions\n",
      "incorrectly classified by the node c. in post-pruning, if the\n",
      "difference of a node’s error rate from before to after split is\n",
      "less than a threshold, the node will be pruned by removing\n",
      "all of its branches to make it a leaf node.\n",
      "\n",
      "4.3\n",
      "\n",
      "the compact set of dt-rl rules\n",
      "\n",
      "in order to induce a compact set of dt-rl rules, we applied the dts to the full set of 3706 rl-induced rules. the\n",
      "induced unweighted and weighted dts without pruning has\n",
      "2527 and 2456 rules (leaf nodes) respectively. thus, without pruning, dts are already able to extract a smaller set\n",
      "of rules: it reduced the total number of rules by over 1000.\n",
      "figure 1 shows the relationship between the number of leaf\n",
      "nodes (x-axis) and the inverted weighted accuracy (y-axis).\n",
      "weighted accuracy(w a) is the weighted percentage of decisions correctlypmade, which can be calculated by the equation: w a =\n",
      "\n",
      "di ∈t\n",
      "\n",
      "p\n",
      "\n",
      "di\n",
      "\n",
      "wi\n",
      "\n",
      "wi\n",
      "\n",
      ". t is the set of correct predictions\n",
      "\n",
      "made by a dt and wi is the weight of decision i. the inverted weighted accuracy (iw a) is iw a = w a−10 , the\n",
      "lower the better. since our goal is to find a good balance\n",
      "point between the iwa and the number of leaf nodes, we\n",
      "applied a widely used strategy called the elbow method,\n",
      "to select the best tree. as we can see in the figure, the\n",
      "elbows for the two unweighted tree approaches are around\n",
      "800 and 1700 rules (x-axis) for the pre and post pruning\n",
      "respectively while the elbows for the two weighted tree approaches are around 250 and 500 for the pre and post pruning respectively. so it seems that weighted tree can extract\n",
      "more compact set of rules than the unweighted trees. while\n",
      "the weighted pre-pruning approach has around 250 rules,\n",
      "its iwa is much higher than the weighted post-pruning approach. therefore, we chose the weighted tree with postpruning strategy which has the an elbow at about 500 leaf\n",
      "nodes and reasonable iwa.\n",
      "to further justify our dt choice, table 1 shows the relationship between the pruning thresholds, w a and the number\n",
      "\n",
      "figure 1: leaf nodes - accuracy\n",
      "of leaf nodes for the weighted tree with post-pruning. table 1 shows that the tree with the closest number of leaves\n",
      "to 500 is the 529 one. it can be obtained by apply a pruning\n",
      "threshold of 0.8 and the result tree has a weighted accuracy\n",
      "of 0.76. the rules in the resulted tree will be the rules used\n",
      "in the dt-rl condition.\n",
      "in short, we applied dt on rl-induced pedagogical policies\n",
      "to extract a more compact set of decision-making rules. the\n",
      "effectiveness of the original full set and the compact set of\n",
      "policies were empirically compared against a baseline policy\n",
      "which makes random yet reasonable decisions: ps vs. we.\n",
      "thus, we have three conditions:\n",
      "1. full-rl: the full set of 3706 rl-induced rules.\n",
      "2. dt-rl: the compact set of 529 dt-induced rl rules.\n",
      "3. random: the random yet reasonable policy.\n",
      "\n",
      "5.\n",
      "\n",
      "empirical experiment\n",
      "\n",
      "participants: this study was conducted in the undergraduate discrete mathematics course at the department\n",
      "of computer science at nc state university in the fall of\n",
      "2016. 153 students participated in this study, which was\n",
      "given as their final homework assignment.\n",
      "conditions: students in the study were assigned to three\n",
      "conditions via balanced random assignment based upon their\n",
      "course section and performance on the class mid-term exam.\n",
      "since the primary goal of this work is to examine the effectiveness of the two rl based policies, we assigned more\n",
      "students to the full-rl and dt-rl conditions than in the\n",
      "random condition. the final group sizes were: n = 61 (fullrl), n = 51 (dt-rl), and n = 41 (random).\n",
      "due to preparations for exams and length of the experiment,\n",
      "126 students completed the experiment. 5 students were\n",
      "excluded from the subsequent analysis due to perfect pretest\n",
      "scores, working in group or gaming the system during the\n",
      "training. the remaining 121 students were distributed as\n",
      "follows: n = 45 for full-rl; n = 41 for rl-dt; n = 35\n",
      "for random. we performed a χ2 test of the relationship\n",
      "between students’ condition and their rate of completion\n",
      "and found no significant difference among the conditions:\n",
      "χ2 (2) = 0.955, p = 0.620.\n",
      "probability tutor: pyrenees is a web-based its for probability. it covers 10 major principles of probability, such\n",
      "as the complement theorem and bayes’ rule. pyrenees116\n",
      "\n",
      "\f",
      "threshold\n",
      "wa\n",
      "leaves\n",
      "\n",
      "table 1: weighted dt with post-pruning\n",
      "0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "1.00 0.99 0.98 0.96 0.93 0.89 0.85 0.79\n",
      "2456 2217 2029 1809 1608 1383 1043 758\n",
      "\n",
      "provides step-by-step instruction and immediate feedback.\n",
      "pyrenees can also provide on-demand hints prompting the\n",
      "student with what they should do next. as with other systems, help in pyrenees is provided via a sequence of increasingly specific hints. the last hint in the sequence, the\n",
      "bottom-out hint, tells the student exactly what to do. for\n",
      "the purposes of this study we incorporated three distinct\n",
      "pedagogical decision modes into pyrenees to match the three\n",
      "conditions.\n",
      "procedure: in this experiment, students were required to\n",
      "complete 4 phases: 1) pre-training, 2) pre-test, 3) training on\n",
      "pyrenees, and 4) post-test. during the pre-training phase,\n",
      "all students studied the domain principles through a probability textbook, reviewed some examples, and solved certain\n",
      "training problems. the students then took a pre-test which\n",
      "contained 14 problems. the textbook was not available at\n",
      "this phase and students were not given feedback on their answers, nor were they allowed to go back to earlier questions.\n",
      "this was also true of the post-test.\n",
      "during phase 3, students in all three conditions received\n",
      "the same 12 rather complicated problems in the same order\n",
      "on pyrenees. each main domain principle was applied at\n",
      "least twice. the minimal number of steps needed to solve\n",
      "each training problem ranged from 20 to 50. these steps\n",
      "included defining variables, applying principles, and solving equations. the number of domain principles required to\n",
      "solve each problem ranged from 3 to 11. all of the students\n",
      "could access the corresponding pre-training textbook during this phase. each step in the problems could have been\n",
      "provided as either a we or ps based upon the condition\n",
      "policy. finally, all of the students completed a post-test\n",
      "with 20 problems. 14 of the problems were isomorphic to\n",
      "the pre-test given in phase 2. the remaining six were nonisomorphic complicated problems.\n",
      "grading criteria: the test problems required students to\n",
      "derive an answer by writing and solving one or more equations. we used three scoring rubrics: binary, partial credit,\n",
      "and one-point-per-principle. under the binary rubric, a solution was worth 1 point if it was completely correct or 0\n",
      "if not. under the partial credit rubric, each problem score\n",
      "was defined by the proportion of correct principle applications evident in the solution. a student who correctly applied 4 of 5 possible principles would get a score of 0.8. the\n",
      "one-point-per-principle rubric in turn gave a point for each\n",
      "correct principle application. all of the tests were graded in\n",
      "a double-blind manner by a single experienced grader. the\n",
      "results presented below are based upon the partial-credit\n",
      "rubric but the same results hold for the other two. for\n",
      "comparison purposes, all test scores were normalized to the\n",
      "range of [0,1].\n",
      "\n",
      "6.\n",
      "\n",
      "0.8\n",
      "0.76\n",
      "529\n",
      "\n",
      "0.9\n",
      "0.68\n",
      "231\n",
      "\n",
      "empirical results\n",
      "\n",
      "since both the full-rl and dt-rl policies are based on an\n",
      "rl-induced policy, we combined the two conditions together\n",
      "as the induced group to evaluate the effectiveness the rlinduced policy. the evaluation was conducted by comparing\n",
      "the induced group with the baseline random condition on\n",
      "learning performance and training time. moreover, in order to further discover to what extent the compact policy\n",
      "retained the power of the full policy, we compared the fullrl and dt-rl conditions on the same measures. next, we\n",
      "will discuss each of the comparisons in turn.\n",
      "\n",
      "6.1 induced vs. random\n",
      "we measured students’ incoming competence via the pretest scores collected before training took place. table 2\n",
      "shows a comparison between the induced group and the\n",
      "random group in terms of learning performance. the parenthesized values following the group names in row 1 denote\n",
      "the number of students in each group. the second row in this\n",
      "table shows the pre-test scores. the last column shows the\n",
      "pairwise t-test results. pairwise t-tests on students’ pre-test\n",
      "scores show that there is no significant difference between\n",
      "the two groups: t(119) = −0.346, p = 0.730, d = 0.069.\n",
      "thus, despite attrition, the two groups remained balanced\n",
      "in terms of incoming competence. next, we will compare the\n",
      "two groups in terms of learning performance in the post-test\n",
      "and training time.\n",
      "rows 2 - 4 in table 2 show a comparison of the pre-test, isomorphic post-test (14 isomorphic questions), and adjusted\n",
      "post-test scores between the two groups along with the mean\n",
      "and sd for each. in order to examine the students’ improvement through training on pyrenees, we compared their\n",
      "scores on the pre-test and isomorphic post-test questions.\n",
      "a repeated measures analysis using test type (pre-test and\n",
      "isomorphic post-test) as factors and test score as the dependent measure showed a main effect for test type: f (1, 119) =\n",
      "98.75, p < 0.0001. further comparisons on group by group\n",
      "basis showed that on the isomorphic questions, both groups\n",
      "scored significantly higher in the post-test than in the pretest: f (1, 85) = 81.30, p < 0.0001 for induced and f (1, 34) =\n",
      "18.30, p = 0.0001 for random respectively. this suggests\n",
      "that the basic practice and problems, domain exposure, and\n",
      "interactivity of our its might help students to learn even\n",
      "when pedagogical decisions are made randomly.\n",
      "in order to investigate the effectiveness of the induced policies, we compared students’ overall learning performance,\n",
      "which was evaluated by their adjusted post-test scores, between the two groups. a one-way ancova analysis was\n",
      "conducted on their overall post-test scores (20 questions),\n",
      "using the pretest scores as a covariate to factor out the influence of their incoming competence. the result shows a\n",
      "significant main effect: f (1, 118) = 4.628, p = 0.033. that\n",
      "is, the induced group significantly outperformed the random group on adjusted post-test scores, which is shown in117\n",
      "\n",
      "\f",
      "cond\n",
      "pre\n",
      "iso post\n",
      "adjusted post\n",
      "time\n",
      "we steps\n",
      "ps steps\n",
      "we pct(%)\n",
      "\n",
      "table 2: induced vs. random\n",
      "induced(86)\n",
      "random(35)\n",
      "t-test result\n",
      ".686(.194)\n",
      ".699(.171)\n",
      "t(119) = −0.346, p = 0.730, d = 0.069\n",
      ".851(.155)\n",
      ".812(.195)\n",
      "t(119) = 1.141, p = 0.256, d = 0.229\n",
      ".751(.144)\n",
      ".689(.138)\n",
      "t(119) = 2.162, p = 0.033, d = 0.433\n",
      "105.87(34.30) 111.18(27.33) t(119) = −0.815, p = 0.417, d = 0.163\n",
      "205.74(62.73) 189.46(11.39)\n",
      "t(119) = 1.522, p = 0.131, d = 0.305\n",
      "173.69(61.14) 190.26(10.28) t(119) = −1.591, p = 0.114, d = 0.319\n",
      "54.16(16.35)\n",
      "49.89(2.78)\n",
      "t(119) = 1.532, p = 0.128, d = 0.307\n",
      "\n",
      "the fourth row of table 2. therefore, the results showed that\n",
      "the induced policies are significantly more effective than the\n",
      "random policy.\n",
      "the fifth row in table 2 shows the average amount of total\n",
      "training time (in minutes) students spent on our its for each\n",
      "group. pairwise t-test showed no significant difference in\n",
      "training time between the two groups: t(119) = −0.815, p =\n",
      "0.417, d = 0.163. the results suggest that when compared\n",
      "to the random policy, the induced policies generally do not\n",
      "have a significant different impact on students’ training time.\n",
      "the last three rows in table 2 show the number of we\n",
      "and ps steps given as well as the percentage of we steps\n",
      "received by the induced and the random group. pairwise\n",
      "t-tests showed that there is no significant difference between\n",
      "the two groups on these three measures.\n",
      "\n",
      "6.2\n",
      "\n",
      "full-rl vs. dt-rl\n",
      "\n",
      "we then performed the same comparison between the fullrl and dt-rl conditions in order to examine the effectiveness of the dt-extracted compact policy. the second row\n",
      "in table 3 shows the pre-test scores for each condition. a\n",
      "pairwise t-test on the scores shows no significant difference\n",
      "between the two conditions: t(84) = −0.168, p = 0.867,\n",
      "d = 0.036. thus the two conditions were balanced in terms\n",
      "of incoming competence.\n",
      "the pre-test, isomorphic post-test and adjusted post-test\n",
      "scores are shown in rows 2 - 4 of table 3. a repeated measures analysis using test type (pre-test and isomorphic posttest) as factors and test score as dependent measure showed\n",
      "a main effect for test type: f (1, 85) = 81.30, p < 0.0001.\n",
      "further comparisons on group by group basis showed that\n",
      "both conditions scored significantly higher in isomorphic\n",
      "post-test than in pre-test: f (1, 44) = 42.16, p < 0.0001\n",
      "for full-rl and f (1, 40) = 39.16, p < 0.0001 for dt-rl.\n",
      "these results suggest that the students can effectively learn\n",
      "from pyrenees with the full and compact policies.\n",
      "in order to discover to what degree the compact policy retained the effectiveness of the full policy, we compared the\n",
      "post-test scores between the two conditions. the results\n",
      "of a pairwise t-test showed no significant different between\n",
      "them on isomorphic post-test: t(84) = 0.505, p = 0.615,\n",
      "d = 0.109. we also conducted an ancova analysis on the\n",
      "overall post-test scores using the pretest scores as a covariate and still found no significant different between the two\n",
      "conditions: f (1, 83) = 0.348, p = 0.557. in short, while on\n",
      "post-test scores, the dt-rl condition scored slightly lower\n",
      "than the full-rl condition, the difference is not significant.\n",
      "\n",
      "the fifth row of table 3 shows the average amount of time\n",
      "students spent on training. as the row shows, the fullrl condition spent significantly more time than the dt-rl\n",
      "condition: t(84) = 3.829, p = 0.0002, d = 0.827. thus\n",
      "the full-rl and dt-rl policies have significant different\n",
      "impact upon the students’ training time.\n",
      "the last three rows of table 3 show the number of we\n",
      "and ps steps given and the percentage of we steps received by the full-rl and the dt-rl condition. pairwise t-tests showed that comparing to the dt-rl condition, the full-rl condition received significantly fewer we\n",
      "steps: t(84) = −4.952, p < 0.0001, d = 1.069; received a\n",
      "lower percentage of we steps: t(84) = −4.955, p < 0.0001,\n",
      "d = 1.070; and completed more ps steps: t(84) = 4.999,\n",
      "p < 0.0001, d = 1.079. these results suggest that the pedagogical decisions made by the compact and full policies are\n",
      "substantively different.\n",
      "\n",
      "7.\n",
      "\n",
      "discussion\n",
      "\n",
      "in this study, we applied dt to extract a compact set of\n",
      "pedagogical rules from the full set of rl-induced rules and\n",
      "empirically evaluated the effectiveness of two sets of rules in\n",
      "a classroom study. our goal was to shed some light on the\n",
      "rl-induced policies and we think this is only the first step\n",
      "towards narrowing the gap and building a bridge between\n",
      "machine-induced pedagogical policies and learning theories.\n",
      "in order to find the best dt, we explored two types of tree:\n",
      "unweighted and weighted; and for each of them, we conducted two types of pruning strategy: pre- and post-pruning.\n",
      "after comparing the performance among them, we selected\n",
      "the weighted tree with the post-pruning strategy to perform\n",
      "the extraction of general decision-making rules. the rlinduced policy contains 3706 specific rules, and the compact\n",
      "dt-rl consisted of 529 rules with a weighted decision accuracy of 76%.\n",
      "in our empirical experiment, we were able to strictly control\n",
      "the domain content and thus to isolate the impact of pedagogy from content. based on this isolation, we compared\n",
      "students’ performance with the full-rl policy, the dt-rl\n",
      "policy and the baseline random policy. our results showed\n",
      "that students in all three conditions learned significantly after training on pyrenees, this suggests that the basic training\n",
      "of the its is effective, even when the pedagogical decisions\n",
      "are made randomly. to evaluate the effectiveness of the two\n",
      "machine induced policies (full-rl policy and dt-rl policy), we combined the full-rl and dt-rl condition as the\n",
      "induced group and compared its learning performance with\n",
      "the random group. our results showed that the induced118\n",
      "\n",
      "\f",
      "cond\n",
      "pre\n",
      "iso post\n",
      "adjusted post\n",
      "time\n",
      "we steps\n",
      "ps steps\n",
      "we pct(%)\n",
      "\n",
      "table 3: full-rl vs. dt-rl\n",
      "full-rl(45)\n",
      "dt-rl (41)\n",
      "t-test result\n",
      ".683(.205)\n",
      ".690(.184)\n",
      "t(84) = −0.168, p = 0.867, d = 0.036\n",
      ".859(.145)\n",
      ".842(.168)\n",
      "t(84) = 0.505, p = 0.615, d = 0.109\n",
      ".757(.144)\n",
      ".739(.145)\n",
      "t(84) = 0.594, p = 0.554, d = 0.128\n",
      "118.42(35.000) 92.10(27.95)\n",
      "t(84) = 3.829, p = 0.0002, d = 0.827\n",
      "177.44(48.86) 236.80(62.03) t(84) = −4.952, p < 0.0001, d = 1.069\n",
      "201.47(47.22) 143.20(60.57)\n",
      "t(84) = 4.999, p < 0.0001, d = 1.079\n",
      "46.77(12.78)\n",
      "62.26(16.13) t(84) = −4.955, p < 0.0001, d = 1.070\n",
      "\n",
      "group significantly outperform the random group. these\n",
      "results suggest that the machine induced policies are indeed\n",
      "more effective than the random policy.\n",
      "finally, in order to examine to what extent the compact dtrl policy retained the power of the full rl-induced policy,\n",
      "we compared the learning performance of the full-rl and\n",
      "the dt-rl conditions. our results suggest that while some\n",
      "of the power was lost in the general rules extraction, the relative performance difference between the full-rl and the\n",
      "dt-rl condition is not significant. in addition, our results\n",
      "on the pedagogical decisions made in training revealed that\n",
      "the compact dt-rl policy selected significant more we\n",
      "than the full-rl policy. this suggests that the two sets\n",
      "of policies indeed made materially different decisions. however, since the weighted dt took account of the importance\n",
      "of each rule, the dt-rl policy aims to retain maximal decision effectiveness from the full-rl policy while the size of\n",
      "the former is less than 15% of the size of the full-rl rules.\n",
      "in the future, we will apply existing learning theories to the\n",
      "decision-making process generated by decision tree to find\n",
      "a theoretical basis for the dt-induced general pedagogical\n",
      "decision-making rules.\n",
      "\n",
      "8.\n",
      "\n",
      "acknowledgements\n",
      "\n",
      "this research was supported by the nsf grant #1432156:\n",
      "“educational data mining for individualized instruction in\n",
      "stem learning environments” and #1651909: “improving\n",
      "adaptive decision making in interactive learning environments”.\n",
      "\n",
      "9.\n",
      "\n",
      "references\n",
      "\n",
      "[1] j. r. anderson, a. t. corbett, k. r. koedinger, and\n",
      "r. pelletier. cognitive tutors: lessons learned. the\n",
      "journal of the learning sciences, 4(2):167–207, 1995.\n",
      "[2] j. beck, b. p. woolf, and c. r. beal. advisor: a\n",
      "machine learning architecture for intelligent tutor\n",
      "construction. aaai/iaai, 2000:552–557, 2000.\n",
      "[3] c. boutilier, r. dearden, and m. goldszmidt.\n",
      "stochastic dynamic programming with factored\n",
      "representations. artificial intelligence, 121(1):49–107,\n",
      "2000.\n",
      "[4] m. chi, k. vanlehn, d. litman, and p. jordan.\n",
      "empirically evaluating the application of\n",
      "reinforcement learning to the induction of effective\n",
      "and adaptive pedagogical strategies. user modeling\n",
      "and user-adapted interaction, 21(1-2):137–180, 2011.\n",
      "[5] l. j. cronbach and r. e. snow. aptitudes and\n",
      "instructional methods: a handbook for research on\n",
      "interactions. irvington, 1977.\n",
      "\n",
      "[6] a. d. davidson and et al. multiple ecological pathways\n",
      "to extinction in mammals. proceedings of the national\n",
      "academy of sciences, 106(26):10702–10705, 2009.\n",
      "[7] u. d. gupta, e. talvitie, and m. bowling. policy tree:\n",
      "adaptive representation for policy gradient. in aaai,\n",
      "pages 2547–2553, 2015.\n",
      "[8] a. iglesias, p. martı́nez, r. aler, and f. fernández.\n",
      "learning teaching strategies in an adaptive and\n",
      "intelligent educational system through reinforcement\n",
      "learning. applied intelligence, 31(1):89–106, 2009.\n",
      "[9] a. iglesias, p. martı́nez, r. aler, and f. fernández.\n",
      "reinforcement learning of pedagogical policies in\n",
      "adaptive and intelligent educational systems.\n",
      "knowledge-based systems, 22(4):266–270, 2009.\n",
      "[10] k. r. koedinger and et al. intelligent tutoring goes to\n",
      "school in the big city. ijaied, 8(1):30–43, 1997.\n",
      "[11] p. phobun and j. vicheanpanya. adaptive intelligent\n",
      "tutoring systems for e-learning systems.\n",
      "procedia-social and behavioral sciences,\n",
      "2(2):4064–4069, 2010.\n",
      "[12] j. r. quinlan. induction of decision trees. machine\n",
      "learning, 1(1):81–106, 1986.\n",
      "[13] s. h. reichard and c. w. hamilton. predicting\n",
      "invasions of woody plants introduced into north\n",
      "america. conservation biology, 11(1):193–203, 1997.\n",
      "[14] s. shen and m. chi. aim low: correlation-based\n",
      "feature selection for model-based reinforcement\n",
      "learning. edm, 2016.\n",
      "[15] s. shen and m. chi. reinforcement learning: the\n",
      "sooner the better, or the later the better? in umap,\n",
      "pages 37–44. acm, 2016.\n",
      "[16] r. s. sutton and a. g. barto. reinforcement learning:\n",
      "an introduction, volume 1. mit press cambridge,\n",
      "1998.\n",
      "[17] k. vanlehn. the behavior of tutoring systems.\n",
      "ijaied, 16(3):227–265, 2006.\n",
      "[18] m. p. vayssières, r. e. plant, and b. h. allen-diaz.\n",
      "classification trees: an alternative non-parametric\n",
      "approach for predicting species distributions. journal\n",
      "of vegetation science, 11(5):679–694, 2000.\n"
     ]
    }
   ],
   "source": [
    "# 2) get all the data from the text files into the \"documents\" list\n",
    "# P.S. make sure you use the 'utf-8' encoding\n",
    "# -*- coding: UTF-8 -*-\n",
    "documents = []\n",
    "\n",
    "for paper in txt_files:\n",
    "    with open(paper) as filename: \n",
    "        contents = filename.read()\n",
    "        documents.append(contents)\n",
    "        \n",
    "# print(documents[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "\n",
      "\f",
      "epistemic network analysis and topic modeling for chat\n",
      "data from collaborative learning environment\n",
      "zhiqiang cai\n",
      "\n",
      "brendan eagan\n",
      "\n",
      "nia m. dowell\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 410\n",
      "memphis, tn, usa\n",
      "\n",
      "university of wisconsin-madison\n",
      "1025 west johnson street\n",
      "madison, wi, usa\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 410\n",
      "memphis, tn, usa\n",
      "\n",
      "zcai@memphis.edu\n",
      "\n",
      "eaganb@gmail.com\n",
      "\n",
      "niadowell@gmail.com\n",
      "\n",
      "james w. pennebaker\n",
      "\n",
      "david w. shaffer\n",
      "\n",
      "arthur c. graesser\n",
      "\n",
      "university of texas-austin\n",
      "116 inner campus dr stop g6000\n",
      "austin, tx, usa\n",
      "\n",
      "university of wisconsin-madison\n",
      "1025 west johnson street\n",
      "madison, wi, usa\n",
      "\n",
      "the university of memphis\n",
      "365 innovation drive, suite 403\n",
      "memphis, tn, usa\n",
      "\n",
      "pennebaker@utexas.edu\n",
      "\n",
      "dws@education.wisc.edu\n",
      "\n",
      "art.graesser@gmail.com\n",
      "\n",
      "abstract\n",
      "this study investigates a possible way to analyze chat data from\n",
      "collaborative learning environments using epistemic network\n",
      "analysis and topic modeling. a 300-topic general topic model\n",
      "built from tasa\n"
     ]
    }
   ],
   "source": [
    "# 3) print the first 1000 characters of the first document to see what it \n",
    "# looks like (we'll use this as a sanity check below)\n",
    "\n",
    "print (documents[0][0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) only select the text that's between the first occurence of the \n",
    "# the word \"abstract\" and the last occurence of the word \"reference\"\n",
    "# Optional: print the length of the string before and after, as a \n",
    "# sanity check\n",
    "# HINT: https://stackoverflow.com/questions/14496006/finding-last-occurrence-of-substring-in-string-replacing-that\n",
    "# read more about rfind: https://www.tutorialspoint.com/python/string_rfind.htm\n",
    "\n",
    "import re\n",
    "\n",
    "documents_clean = []\n",
    "\n",
    "for doc in documents:\n",
    "    abstract = doc.find(\"abstract\")\n",
    "    ref = doc.rfind(\"reference\")\n",
    "    new_string = doc[abstract+8:ref] \n",
    "    #could have done length of abstract...or split the string after the len(abstract)...what's most efficient?\n",
    "#     print(new_string)\n",
    "#     print(\"doc \"+ str(len(doc)))\n",
    "#     print(\"new_string \" + str(len(new_string)))\n",
    "    documents_clean.append(new_string)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# 5) replace carriage returns (i.e., \"\\n\") with a white space\n",
    "# check that the result looks okay by printing the \n",
    "# first 1000 characters of the 1st doc:\n",
    "\n",
    "documents_clean_white = []\n",
    "\n",
    "for doc in documents_clean:\n",
    "    doc = doc.replace(\"\\n\",\" \")\n",
    "    documents_clean_white.append(doc)\n",
    "#     print(doc)\n",
    "\n",
    "documents_clean_white[0][0:1000]\n",
    "\n",
    "print(len(documents_clean_white))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' this study investigates a possible way to analyze chat data from collaborative learning environments using epistemic network analysis and topic modeling  a 300 topic general topic model built from tasa  touchstone applied science associates  corpus was used in this study  300 topic scores for each of the 15 670 utterances in our chat data were computed  seven relevant topics were selected based on the total document scores  while the aggregated topic scores had some power in predicting students  learning  using epistemic network analysis enables assessing the data from a different angle  the results showed that the topic score based epistemic networks between low gain students and high gain students were significantly different  𝑡   2 00   overall  the results suggest these two analytical approaches provide complementary information and afford new insights into the processes related to successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epist'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) replace the punctation below by a white space\n",
    "# check that the result looks okay \n",
    "# (e.g., by print the first 1000 characters of the 1st doc)\n",
    "\n",
    "punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '−', '”', '“', '’']\n",
    "\n",
    "docs_clean_punct= []\n",
    "\n",
    "for doc in documents_clean_white:\n",
    "    for i in punctuation:\n",
    "        doc = doc.replace(i, \" \")\n",
    "    docs_clean_punct.append(doc)\n",
    "\n",
    "docs_clean_punct[0][0:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' this study investigates a possible way to analyze chat data from collaborative learning environments using epistemic network analysis and topic modeling  a   topic general topic model built from tasa  touchstone applied science associates  corpus was used in this study    topic scores for each of the     utterances in our chat data were computed  seven relevant topics were selected based on the total document scores  while the aggregated topic scores had some power in predicting students  learning  using epistemic network analysis enables assessing the data from a different angle  the results showed that the topic score based epistemic networks between low gain students and high gain students were significantly different  𝑡         overall  the results suggest these two analytical approaches provide complementary information and afford new insights into the processes related to successful collaborative interactions   keywords chat  collaborative learning  topic modeling  epistemic net'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7) remove numbers by either a white space or the word \"number\"\n",
    "# again, print the first 1000 characters of the first document\n",
    "# to check that you're doing the right thing\n",
    "\n",
    "docs_clean_num = []\n",
    "\n",
    "for doc in docs_clean_punct:\n",
    "    doc = re.sub('\\w*[0-9]\\w*', \" \",doc)\n",
    "    docs_clean_num.append(doc)\n",
    "    \n",
    "docs_clean_num[0][0:1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "# 8) Remove the stop words below from our documents\n",
    "# print the first 1000 characters of the first document\n",
    "\n",
    "# https://stackoverflow.com/questions/25346058/removing-list-of-words-from-a-string\n",
    "\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', \n",
    "              'just', 'don', 'should', 'now']\n",
    "\n",
    "docs_clean_stop = []\n",
    "\n",
    "for doc in docs_clean_num:\n",
    "    docwords = doc.split()\n",
    "    resultwords  = [word for word in docwords if word.lower() not in stop_words]\n",
    "    result = ' '.join(resultwords)\n",
    "    docs_clean_stop.append(result)\n",
    "\n",
    "docs_clean_stop[0][0:1000]\n",
    "\n",
    "print(len(docs_clean_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling topic general topic model built tasa touchstone applied science associates corpus used study topic scores utterances chat data computed seven relevant topics selected based total document scores aggregated topic scores power predicting students learning using epistemic network analysis enables assessing data different angle results showed topic score based epistemic networks low gain students high gain students significantly different overall results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions keywords chat collaborative learning topic modeling epistemic network analysis introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synchronously asynchronously participate tasks accom'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9) remove words with one and two characters (e.g., 'd', 'er', etc.)\n",
    "# print the first 1000 characters of the first document\n",
    "\n",
    "docs_clean_short = []\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "\n",
    "for doc in docs_clean_stop:\n",
    "    doc = shortword.sub('',doc)\n",
    "    docs_clean_short.append(doc)\n",
    "    \n",
    "docs_clean_short[0][0:1000]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Putting it all together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** VERSION ONE***\n",
    "\n",
    "# 10) package all of your work above into a function that cleans a given document\n",
    "\n",
    "#store body of paper from abstract to reference\n",
    "def paperbody(documents):\n",
    "    docs_temp = []\n",
    "    for doc in documents:\n",
    "        abstract = doc.find(\"abstract\")\n",
    "        ref = doc.rfind(\"reference\")\n",
    "        new_string = doc[abstract+8:ref] \n",
    "        docs_temp.append(new_string)\n",
    "#     print(docs_temp[0][0:1000])\n",
    "    return docs_temp\n",
    "\n",
    "# whitespace\n",
    "def whitespace(documents):\n",
    "    documents_clean_white = []\n",
    "    for doc in documents_clean:\n",
    "        doc = doc.replace(\"\\n\",\" \")\n",
    "        documents_clean_white.append(doc)\n",
    "    return documents_clean_white\n",
    "\n",
    "#punctuation\n",
    "def punctuation(documents):\n",
    "    punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '−', '”', '“', '’']\n",
    "    docs_clean_punct= []\n",
    "\n",
    "    for doc in documents_clean_white:\n",
    "        for i in punctuation:\n",
    "            doc = doc.replace(i, \" \")\n",
    "        docs_clean_punct.append(doc)\n",
    "    return docs_clean_punct\n",
    "\n",
    "# numbers\n",
    "def numbers(documents):\n",
    "    docs_clean_num = []\n",
    "    for doc in docs_clean_punct:\n",
    "        doc = re.sub('\\w*[0-9]\\w*', \" \",doc)\n",
    "        docs_clean_num.append(doc)\n",
    "    return docs_clean_num\n",
    "\n",
    "#stopwords\n",
    "def stopwords(documents):\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', \n",
    "              'just', 'don', 'should', 'now']\n",
    "    docs_clean_stop = []\n",
    "    for doc in docs_clean_num:\n",
    "        docwords = doc.split()\n",
    "        resultwords  = [word for word in docwords if word.lower() not in stop_words]\n",
    "        result = ' '.join(resultwords)\n",
    "        docs_clean_stop.append(result)\n",
    "    return docs_clean_stop\n",
    "\n",
    "def shortwords(documents):\n",
    "    docs_clean_short = []\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    for doc in docs_clean_stop:\n",
    "        doc = shortword.sub('',doc)\n",
    "        docs_clean_short.append(doc)\n",
    "    return docs_clean_short\n",
    "\n",
    "def clean_list_of_documents(documents):\n",
    "    \n",
    "    cleaned_docs = []\n",
    "\n",
    "    ### your code ###\n",
    "\n",
    "    cleaned_docs = paperbody(documents)\n",
    "    cleaned_docs = whitespace(documents)\n",
    "    cleaned_docs = punctuation(documents)\n",
    "    cleaned_docs = numbers(documents)\n",
    "    cleaned_docs = stopwords(documents)\n",
    "    cleaned_docs = shortwords(documents)\n",
    "    \n",
    "    return cleaned_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Version TWO***\n",
    "# Yes I know this is shorter but I wrote the first version first\n",
    "\n",
    "def clean_list_of_documents2(documents):    \n",
    "    cleaned_docs = []\n",
    "    stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "              'ourselves', 'you', 'your', 'yours', 'yourself', \n",
    "              'yourselves', 'he', 'him', 'his', 'himself', 'she', \n",
    "              'her', 'hers', 'herself', 'it', 'its', 'itself', \n",
    "              'they', 'them', 'their', 'theirs', 'themselves', \n",
    "              'what', 'which', 'who', 'whom', 'this', 'that', \n",
    "              'these', 'those', 'am', 'is', 'are', 'was', 'were', \n",
    "              'be', 'been', 'being', 'have', 'has', 'had', 'having', \n",
    "              'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', \n",
    "              'but', 'if', 'or', 'because', 'as', 'until', 'while', \n",
    "              'of', 'at', 'by', 'for', 'with', 'about', 'against', \n",
    "              'between', 'into', 'through', 'during', 'before', \n",
    "              'after', 'above', 'below', 'to', 'from', 'up', 'down', \n",
    "              'in', 'out', 'on', 'off', 'over', 'under', 'again', \n",
    "              'further', 'then', 'once', 'here', 'there', 'when', \n",
    "              'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "              'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "              'nor', 'not', 'only', 'own', 'same', 'so', 'than', \n",
    "              'too', 'very', 's', 't', 'can', 'will', \n",
    "              'just', 'don', 'should', 'now']\n",
    "    punctuation = ['.', '...', '!', '#', '\"', '%', '$', \"'\", '&', ')', \n",
    "               '(', '+', '*', '-', ',', '/', '.', ';', ':', '=', \n",
    "               '<', '?', '>', '@', '\",', '\".', '[', ']', '\\\\', ',',\n",
    "               '_', '^', '`', '{', '}', '|', '~', '−', '”', '“', '’']\n",
    "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "    for doc in documents:\n",
    "        abstract = doc.find(\"abstract\")\n",
    "        ref = doc.rfind(\"reference\")\n",
    "        new_string = doc[abstract+8:ref] \n",
    "        doc = new_string.replace(\"\\n\",\" \")\n",
    "        for i in punctuation:\n",
    "            doc = doc.replace(i, \" \")\n",
    "        doc = re.sub('\\w*[0-9]\\w*', \" \",doc)\n",
    "        docwords = doc.split()\n",
    "        resultwords  = [word for word in docwords if word.lower() not in stop_words]\n",
    "        result = ' '.join(resultwords)   \n",
    "        doc = shortword.sub('',result)\n",
    "        cleaned_docs.append(doc)\n",
    "    return cleaned_docs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling topic general topic model built tasa touchstone applied science associates corpus used study topic scores utterances chat data computed seven relevant topics selected based total document scores aggregated topic scores power predicting students learning using epistemic network analysis enables assessing data different angle results showed topic score based epistemic networks low gain students high gain students significantly different overall results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions keywords chat collaborative learning topic modeling epistemic network analysis introduction collaborative learning special form learning interaction affords opportunities groups students combine cognitive resources synchronously asynchronously participate tasks accom\n"
     ]
    }
   ],
   "source": [
    "# 11a) reimport your raw data using the code in 2)\n",
    "documents = []\n",
    "\n",
    "for paper in txt_files:\n",
    "    with open(paper) as filename: \n",
    "        contents = filename.read()\n",
    "        documents.append(contents)\n",
    "\n",
    "        \n",
    "# 11b) clean your files using the function above\n",
    "# clean_docs = clean_list_of_documents(documents)\n",
    "\n",
    "clean_docs2 = clean_list_of_documents2(documents)\n",
    "\n",
    "\n",
    "# 11c) print the first 1000 characters of the first document\n",
    "\n",
    "# print(clean_docs[0][:1000])\n",
    "\n",
    "print(clean_docs2[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Build your list of vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of words (i.e., the vocabulary) is going to become the columns of your matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Describe why we need to figure out the vocabulary used in our corpus (refer back to Sherin's paper, and explain in your own words): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5639\n"
     ]
    }
   ],
   "source": [
    "# 13) create a function that takes in a list of documents\n",
    "# and returns a set of unique words. Make sure that you\n",
    "# sort the list alphabetically before returning it. \n",
    "\n",
    "def get_vocabulary(documents):\n",
    "    voc = []\n",
    "    for doc in documents:\n",
    "        docList = doc.split()\n",
    "        for word in docList:\n",
    "            if word not in voc:\n",
    "                voc.append(word)\n",
    "    return voc\n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "for paper in txt_files:\n",
    "    with open(paper) as filename: \n",
    "        contents = filename.read()\n",
    "        documents.append(contents)\n",
    "        \n",
    "vocabulary = get_vocabulary(clean_docs2)\n",
    "\n",
    "print(len(vocabulary))\n",
    "\n",
    "# print(vocabulary)\n",
    "\n",
    "# Then print the length of your vocabulary (it should be \n",
    "# around 5500 words)\n",
    "\n",
    "# ^^ assuming the cleaned dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) what was the size of Sherin's vocabulary? \n",
    "\n",
    "# \"For the corpus used in this work, the full vocabulary contained\n",
    "# 1,429 words, the stop list consisted of 782 words, and the resulting pruned vocabulary\n",
    "# contained 647 words.\" (p.617)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - transform your documents into 100-words chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) create a function that takes in a list of documents\n",
    "# and returns a list of 100-words chunk \n",
    "# (with a 25 words overlap between them)\n",
    "# Optional: add two arguments, one for the number of words\n",
    "# in each chunk, and one for the overlap size\n",
    "# Advice: combining all the documents into one giant string\n",
    "# and splitting it into separate words will make your life easier!\n",
    "\n",
    "\n",
    "docs_all = ''.join(clean_docs2)\n",
    "\n",
    "# print(docs_all)\n",
    "\n",
    "def wordChunk (listName, chunkSize, overlap):\n",
    "    HundList = []\n",
    "    docsList = listName.split()\n",
    "    for i in range(0, len(docsList), chunkSize-overlap):\n",
    "#         print(i)\n",
    "        HundList.append(\" \".join(docsList[i:i+chunkSize]))\n",
    "#         print(i+chunkSize)\n",
    "    return HundList\n",
    "\n",
    "docs_all = wordChunk(docs_all, 100, 25)\n",
    "\n",
    "# print(docs_all[0])\n",
    "# print(docs_all[1])\n",
    "# print(docs_all[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) create a for loop to double check that each chunk has \n",
    "# a length of 100\n",
    "# Optional: use assert to do this check\n",
    "\n",
    "for chunk in docs_all:\n",
    "    chunk = chunk.split()\n",
    "#     print (len(chunk))\n",
    "#     assert len(chunk) == True\n",
    "#     print (len(chunk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study investigates possible way analyze chat data collaborative learning environments using epistemic network analysis topic modeling topic general topic model built tasa touchstone applied science associates corpus used study topic scores utterances chat data computed seven relevant topics selected based total document scores aggregated topic scores power predicting students learning using epistemic network analysis enables assessing data different angle results showed topic score based epistemic networks low gain students high gain students significantly different overall results suggest two analytical approaches provide complementary information afford new insights processes related successful collaborative interactions keywords chat collaborative learning topic modeling epistemic network analysis\n"
     ]
    }
   ],
   "source": [
    "# 17) print the first chunk, and compare it to the original text.\n",
    "# does that match what Sherin describes in his paper?\n",
    "\n",
    "print(docs_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) how many chunks did Sherin have? What does a chunk become \n",
    "# in the next step of our topic modeling algorithm? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) what are some other preprocessing steps we could do \n",
    "# to improve the quality of the text data? Mention at least 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20) in your own words, describe the next steps of the \n",
    "# data modeling algorithms (listed below):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Vector and Matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Weight word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Matrix normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Deviation Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Visualizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Step - Putting it all together: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in python code, our goal is to recreate the steps above as functions\n",
    "# so that we can just one line to run topic modeling on a list of \n",
    "# documents: \n",
    "def ExtractTopicsVSM(documents, numTopics):\n",
    "    ''' this functions takes in a list of documents (strings), \n",
    "        runs topic modeling (as implemented by Sherin, 2013)\n",
    "        and returns the clustering results, the matrix used \n",
    "        for clustering a visualization '''\n",
    "    \n",
    "    # step 2: clean up the documents\n",
    "    documents = clean_list_of_documents(documents)\n",
    "    \n",
    "    # step 3: let's build the vocabulary of these docs\n",
    "    vocabulary = get_vocabulary(documents)\n",
    "    \n",
    "    # step 4: we build our list of 100-words overlapping fragments\n",
    "    documents = flatten_and_overlap(documents)\n",
    "    \n",
    "    # step 5: we convert the chunks into a matrix\n",
    "    matrix = docs_by_words_matrix(documents, vocabulary)\n",
    "    \n",
    "    # step 6: we weight the frequency of words (count = 1 + log(count))\n",
    "    matrix = one_plus_log_mat(matrix, documents, vocabulary)\n",
    "    \n",
    "    # step 7: we normalize the matrix\n",
    "    matrix = normalize(matrix)\n",
    "    \n",
    "    # step 8: we compute deviation vectors\n",
    "    matrix = transform_deviation_vectors(matrix, documents)\n",
    "    \n",
    "    # step 9: we apply a clustering algorithm to find topics\n",
    "    results_clustering = cluster_matrix(matrix)\n",
    "    \n",
    "    # step 10: we create a visualization of the topics\n",
    "    visualization = visualize_clusters(results_clustering, vocabulary)\n",
    "    \n",
    "    # finally, we return the clustering results, the matrix, and a visualization\n",
    "    return results_clustering, matrix, visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
